// lex.torth - Lexing functions that parses Tokens from code files
include "std"
include "typing"
include "compiler/defs"
include "compiler/utils"
include "compiler/class/Constant"
include "compiler/class/Function"
include "compiler/class/Match"
include "compiler/class/Memory"
include "compiler/class/Op"
include "compiler/class/Token"
include "compiler/class/Signature"
include "compiler/class/Variable"

// Parse included files from a code string. Return the list of files.
// Params: code_file (STR)
// Return: List[str]
function get_included_files str -> ptr :
  ptr List.init
  peek included_files in List.append

  // Iterate over all of the new included files for this loop iteration
  0 take index in
  while index included_files List.len < do

    // Save the current file name
    index included_files List.nth str.load
    take file_name in

    // Get includes from the current file
    included_files file_name get_included_files_from_file

    // Save the new included files from this iteration
    included_files =
    index 1 + index = // index++
  done
  included_files
end

// Parse included files from a single file. Return the list of files.
// Params: file_name (STR), included_files (List[str])
// Return: included_files (List[str])
function get_included_files_from_file str ptr -> ptr :
  // Add compiler's directory and the 'lib' directory to INCLUDE_PATHS
  List.init
  take INCLUDE_PATHS in

  ""      ptr INCLUDE_PATHS List.append
  "lib/"  ptr INCLUDE_PATHS List.append
  take
    code_file
    included_files
  in

  // Get the file contents without comments
  INCLUDE_PATHS code_file get_file_name_from_path read_file
  List.init code_file rot get_token_matches_from_code
  False
  0
  take
    index
    parsing_include
    token_matches
  in

  while index token_matches List.len < do
    // Get Nth word from words list
    index token_matches List.nth ptr.load
    Match.get_value
    take token_value in

    // Parse the file name for the include
    if parsing_include do
      token_value take file_name in

      if file_name int NULL == do
        "INCLUDE keyword used without given file name\n"
        "INCLUDE_ERROR" CompilerError
      endif

      // Get the file name that exists using directories in PATH
      INCLUDE_PATHS file_name get_file_name_from_path
      file_name =

      // Do not save file name if it already is in the parsing_include list
      if file_name included_files List.contains_str do
        False parsing_include =
        index 1 + index =
        continue
      endif

      // Save the file name to included_files
      file_name ptr included_files List.append
      False parsing_include =
    endif

    if token_value str.copy str.upper "INCLUDE" streq do
      True parsing_include =
    endif
    index 1 + index = // index++
  done
  included_files
end

// Check every directory in PATH and add .torth extension if needed
// Params: file_name: str, INCLUDE_PATHS: List[str]
// Return: file_with_path: str
function get_file_name_from_path str ptr -> str :
  take file_name INCLUDE_PATHS in

  // Get the file name without quotes
  if file_name ptr char.load '"' == do
    file_name get_string_inside_quotes
    file_name =
  endif

  if file_name int NULL == do
    "Cannot include file '"
    file_name str.cat
    "'.\n"      str.cat
    "INCLUDE_ERROR" CompilerError
  endif

  if file_name file_exists do
    file_name return
  endif

  // Iterate over INCLUDE PATHS
  // and check if file_name or file_with_extension exists
  0 take include_index in
  while include_index INCLUDE_PATHS List.len < do
    include_index INCLUDE_PATHS List.nth str.load
    take path_dir in

    // Append file_name to path_dir and check if it exists
    path_dir file_name str.cat
    take file_with_path in
    if file_with_path file_exists do
      file_with_path return
    else
      // Check if the file with .torth extension exists
      file_with_path ".torth" str.cat
      file_with_path =
      if file_with_path file_exists do
        file_with_path return
      endif
    endif
    include_index 1 + include_index =
  done

  "File '" file_name       str.cat
  "' cannot be included.\n" str.cat
  "INCLUDE_ERROR" CompilerError ""
end

// Get Token Matches from the code inside included files
// Params:
//    included_files: List[str]
// Return:
//    token_matches: List[Match]
function get_token_matches_from_files ptr -> ptr :
  List.init
  0
  take
    index
    token_matches
    included_files
  in

  // Iterate over each included file
  while index included_files List.len < do

    // Read the current file and remove its comments
    index included_files List.nth str.load
    dup read_file
    take code file_name in

    // Parse Token Matches from the current file's code
    token_matches file_name code get_token_matches_from_code
    token_matches =
    index 1 + index = // index++
  done
  token_matches
end

// Get multiline string inside double quotes ""
// Return NULL if there is no string inside quotes
// Params: String which begins with a double quote
// Return: The multiline string between double quotes
function get_string_inside_quotes str -> str :
  str.copy
  take original in

  // Return NULL if the first character is not a double quote
  if original ptr char.load '"' != do
    NULLPTR str return
  endif

  1 while dup original str.len < do
    1 + // index++

    // Return when the other quote is found
    if original over str.char_at '"' == do
      original ptr over ptr+ NULL char swap char.store
      drop
      original 1 str+ return
    endif
  done drop
  NULLPTR str
end

// Transfer comparison and calculations related symbols to their text counterparts
// Params: Token match from code (STR)
// Return: Value for Token (STR)
function get_token_value str -> str :
  str.copy
  take token_match in
  if   token_match "="  streq do
    "ASSIGN" return
  elif token_match "==" streq do
    "EQ" return
  elif token_match ">=" streq do
    "GE" return
  elif token_match ">"  streq do
    "GT" return
  elif token_match "<=" streq do
    "LE" return
  elif token_match "<"  streq do
    "LT" return
  elif token_match "-"  streq do
    "MINUS" return
  elif token_match "*"  streq do
    "MUL" return
  elif token_match "!=" streq do
    "NE" return
  elif token_match "+"  streq do
    "PLUS" return
  endif
  token_match str.escape
end

const single_quote 39 end
function get_token_type str -> int :
  take token_value in
  // TODO: TokenType.KEYWORD
  // Boolean => True
  if
    token_value str.copy str.upper
    dup   "TRUE"  streq
    swap  "FALSE" streq
    ||
  do
    TokenType.BOOL return
  // Character => 'a'
  elif
    token_value str.len 3 ==
    token_value ptr char.load single_quote ==
    token_value 2 str.char_at single_quote ==
    && &&
  do
    TokenType.CHAR return
  // Integer => 1337
  elif
    token_value str.is_numeric
  do
    TokenType.INT return
  // Hexadecimal => 0x1337
  elif
    token_value ptr char.load '0' ==
    token_value 1 str.char_at 'x' ==
    &&
  do
    TokenType.INT return
  // String => "This is string\n"
  elif
    token_value ptr char.load                '"' ==
    token_value dup str.len 1 - str.char_at  '"' ==
    &&
  do
  TokenType.STR return
  // uint8 => u69
  elif
    token_value ptr char.load 'u' ==
    token_value 1 str.char_at char.is_numeric
    &&
  do
    TokenType.UINT8 return
  endif
  TokenType.WORD
end

// Generate Token from Match object
// Params: Match
// Return: Token
function get_token_from_match ptr -> ptr :
  dup   Match.get_value
  swap  Match.get_location
  take location match_value in

  // Create Token object
  location
  match_value get_token_value
  dup get_token_type
  swap Token.init
end

function get_op_type ptr -> int :
  Token.get_type
  take token_type in
  if token_type TokenType.BOOL == do
    OpType.PUSH_BOOL return
  elif token_type TokenType.CHAR == do
    OpType.PUSH_CHAR return
  elif token_type TokenType.INT == do
    OpType.PUSH_INT return
  elif token_type TokenType.STR == do
    OpType.PUSH_STR return
  elif token_type TokenType.UINT8 == do
    OpType.PUSH_UINT8 return
  endif
  OpType.INTRINSIC
end

// Functions are made of four parts:
//   1 : name
//   2 : param types
//   3 : return types
//   4 : location
//  (0 : Not lexing a function)
//  FUNCTION_PART_DELIMITERS: List[str] = ["FUNCTION", "->", ":", "END"]
function get_function_part_delimiters -> ptr :
  List.init
  take FUNCTION_PART_DELIMITERS in

  "FUNCTION"  ptr FUNCTION_PART_DELIMITERS List.append
  ""          ptr FUNCTION_PART_DELIMITERS List.append
  "->"        ptr FUNCTION_PART_DELIMITERS List.append
  ":"         ptr FUNCTION_PART_DELIMITERS List.append
  "END"       ptr FUNCTION_PART_DELIMITERS List.append
  FUNCTION_PART_DELIMITERS
end

// Get all words from code to a list
// Params
//    code: str
//    file_name: str
//    token_matches: List[Token]
// Return
//    token_matches: List[Match]
function get_token_matches_from_code str str ptr -> ptr :
  List.init   // newline_indexes
  "" str.copy // String buffer for word
  False       // parsing_string
  False       // parsing_comment
  10          // Line feed character
  0           // index
  take
    index
    LF
    parsing_comment
    parsing_string
    word
    newline_indexes
    code
    file_name
    token_matches
  in

  // Iterate over every character in code
  while
    code index str.char_at
    peek current_char in
    NULL !=
  do
    // Keep track of the indexes of newline characters (LF)
    if current_char LF == do
      index ptr newline_indexes List.append
    endif

    // Only a newline ends a comment
    if parsing_comment do
      if current_char LF == do
        False parsing_comment =
      endif
      index 1 + index =
      continue
    endif

    // Only a double quote ends a string
    if parsing_string do
      if current_char '"' == do
        False parsing_string =
      endif
      current_char word str.append
      index 1 + index =
      continue
    endif

    // Double quote starts a string
    if
      current_char '"' ==
      word "'" streq not  // Double quote is not in character definition => '"'
      &&
    do
      True parsing_string =
      current_char word str.append
      index 1 + index =
      continue
    endif

    // Whitespaces separate Tokens
    if
      current_char char.is_whitespace
    do
      // If the word is empty there has been multiple whitespaces in a row
      if word str.len 0 == do
        index 1 + index = // index++
        continue
      endif

      // Check for comments
      if word "//" str.startswith do
        True parsing_comment =
        "" str.copy word =
        index 1 + index =
        continue
      endif

      // Check if the whitespace is inside character definition => ' '
      if word "'" streq do
        current_char word str.append
        index 1 + index =
        continue
      endif

      // Match(word, location)
      newline_indexes index word str.len - file_name get_token_location
      word Match.init
      token_matches List.append
      "" str.copy word =
    else
      // Append current character to word
      current_char word str.append
    endif
    index 1 + index = // index++
  done

  // Append the last word if the file ended with a word
  if word str.len 0 > do
    newline_indexes index word str.len - file_name get_token_location
    word Match.init
    token_matches List.append
  endif
  token_matches
end

// Calculate position for Token based on it's position from the start of the file
// and the indexes of the newline characters found from the file.
// Params: file_name (STR), position (INT), newline_indexes (List[Int])
// Return: Location
function get_token_location str int ptr -> ptr :
  over
  1
  0
  take
    index
    row
    col
    file_name
    position
    newline_indexes
  in

  // Get Location for found Token Match
  while index newline_indexes List.len < do

    if index 0 > do
      index 1 - newline_indexes List.nth int.load
      take nl_index in

      row 1 + row = // row++
      position nl_index - 1 - col = // col = position - nl_index - 1
    endif

    // Return when the current Token Location is found
    if index newline_indexes List.nth int.load position > do
      col 1 + row file_name str.copy Location.init
      return
    endif
    index 1 + index =
  done

  col 1 + row file_name str.copy Location.init
end

// Parse Constants from code
// Params
//    token_matches: List[Match], constants: List[Constant]
// Return
//    constants: List[Constant]
function get_constants ptr -> ptr :
  List.init
  ""  str.copy        // constant_name
  dup str.copy        // constant_value
  False               // defining_constant
  0                   // index
  take
    index
    defining_constant
    constant_value
    constant_name
    constants
    token_matches
  in

  while index token_matches List.len < do
    index token_matches List.nth ptr.load
    dup   Match.get_value
    swap  Match.get_location
    take location token_value in

    // Parse Constant only when inside Constant block
    if defining_constant do

      // Parse Constant name
      if constant_name str.len 0 == do
        constants token_value get_constants_name
        constant_name =
        index 1 + index = // index++
        continue
      endif

      // Append the current Constant to constants
      if constant_value str.len 0 == do

        // Parse Constant value
        token_value constant_value =

        // Generate Constant object
        location constant_value constant_name Constant.init
        constants List.append

        // Reset variables
        ""    constant_name   =
        ""    constant_value  =
        False defining_constant =
        index 1 + index = // index++
        continue
      endif
    endif

    if token_value str.copy str.upper "CONST" streq do
      True defining_constant =
    endif
    index 1 + index = // index++
  done
  constants token_matches add_enums_to_constants
end

// Parse and add ENUM block contents to a list of Constants.
// Items inside ENUM blocks are interpreted as running integers starting from 0.
// Params
//    token_matches: List[Match]
//    constants: List[Constant]
// Return
//    constants: List[Constant]
function add_enums_to_constants ptr ptr -> ptr :
  get_enum_part_delimiters
  "" str.copy
  NULLPTR
  0
  0
  0
  0
  take
    index
    current_part
    enum_size
    enum_offset
    enum_location
    enum_name
    ENUM_PART_DELIMITERS
    token_matches
    constants
  in

  while index token_matches List.len < do

    // Get the current Token Matches value
    index token_matches List.nth ptr.load
    dup  Match.get_value
    swap Match.get_location

    take location token_value in

    if
      token_value str.copy str.upper
      current_part ENUM_PART_DELIMITERS List.nth str.load
      streq
    do
      current_part 1 +
      ENUM_PART_DELIMITERS List.len %
      current_part =

      if token_value str.copy str.upper "END" streq do

        // Constant(enum_name, enum_size*enum_offset, enum_location)
        enum_location
        enum_size enum_offset * itoa
        enum_name
        Constant.init

        // Append the current Enum to the list of Constants
        constants List.append
        index 1 + index =

        // Reset variables
        0 enum_size =
        continue
      endif

    elif current_part 1 == do
      token_value enum_name =
      location enum_location =
      current_part 1 + current_part =

    elif current_part 2 == do
      if token_value str.is_numeric not do
        "'" token_value str.cat
        "' is not a valid offset parameter for ENUM block\n" str.cat
        "VALUE_ERROR" CompilerError
      endif
      token_value atoi enum_offset =
      current_part 1 + current_part =

    elif current_part 3 == do
      "Token '" token_value str.cat
      "' is used in the wrong context when defining '" str.cat
      enum_name str.cat
      "' Enum.\nCheck the syntax of the Enum definition.\n" str.cat
      "VALUE_ERROR" CompilerError

    elif current_part 4 == do
      if constants token_value constant_exists do
        "Constant '" token_value str.cat
        "' is defined multiple times.\n" str.cat
        "Constant names should be unique.\n" str.cat
        "CONST_REDEFINITION" CompilerError
      endif

      // Constant(token_value, enum_size*enum_offset, location)
      location
      enum_size enum_offset * itoa
      token_value
      Constant.init

      // Append the current Enum item to constants
      constants List.append
      enum_size 1 + enum_size =
    endif
    index 1 + index = // index++
  done
  constants
end

// Enums are made of four parts:
//   1 : name
//   2 : size
//   3 : offset
//   4 : items
//  (0 : Not lexing Enum)
//  ENUM_PART_DELIMITERS: List[str] = ["ENUM", "", "", ":", "END"]
function get_enum_part_delimiters -> ptr :
  List.init
  take ENUM_PART_DELIMITERS in
  "ENUM"  ptr ENUM_PART_DELIMITERS List.append
  ""      ptr ENUM_PART_DELIMITERS List.append
  ""      ptr ENUM_PART_DELIMITERS List.append
  ":"     ptr ENUM_PART_DELIMITERS List.append
  "END"   ptr ENUM_PART_DELIMITERS List.append
  ENUM_PART_DELIMITERS
end

// Parse Memories from code
// Params
//    token_matches: List[Match]
//    constants: List[Constant]
// Return
//    memories: List[Memory]
function get_memories ptr ptr -> ptr :
  List.init           // memories
  "" str.copy         // memory_name
  NULL                // memory_size
  False               // defining_memory
  0                   // index
  take
    index
    defining_memory
    memory_size
    memory_name
    memories
    token_matches
    constants
  in

  while index token_matches List.len < do
    index token_matches List.nth ptr.load
    dup   Match.get_value
    swap  Match.get_location
    take location token_value in

    // Parse Memory only when inside Memory block
    if defining_memory do

      // Parse Memory name
      if memory_name str.len 0 == do
        memories token_value get_memory_name
        memory_name =

        index 1 + index = // index++
        continue
      endif

      // Append the current Memory to memories
      if memory_size 0 == do

        // Use the token_value as memory_size if it is numeric
        if token_value str.is_numeric do
          token_value atoi memory_size =
        else
          // Test if Constant exists
          if constants token_value constant_exists do
            constants token_value get_constant
            Constant.get_value atoi
            memory_size =
          else
            "The memory size should be an integer. Got: "
            token_value str.cat
            "VALUE_ERROR" CompilerError
          endif
        endif

        // Generate Memory object
        location memory_size memory_name str.copy Memory.init
        memories List.append

        // Reset variables
        ""    memory_name =
        NULL  memory_size =
        False defining_memory =
        index 1 + index = // index++
        continue
      endif
    endif

    if token_value str.copy str.upper "MEMORY" streq do
      True defining_memory =
    endif
    index 1 + index = // index++
  done
  memories
end

// Check for redefinition of a Constant object
// Params
//    token_value: str
//    memories: List[Constant]
// Return
//    memory_name: str
function get_constants_name str ptr -> str :
  take token_value constants in

  // Overwriting another Memory is not allowed
  if token_value constants List.contains_str do
    "Constant '" token_value str.cat
    "' already exists. Memory name should be unique.\n" str.cat
    "MEMORY_REDEFINITION" CompilerError
  endif
  token_value
end

// Check for redefinition of a Memory object
// Params
//    token_value: str
//    memories: List[Memory]
// Return
//    memory_name: str
function get_memory_name str ptr -> str :
  take token_value memories in

  // Overwriting another Memory is not allowed
  if token_value memories List.contains_str do
    "Memory '" token_value str.cat
    "' already exists. Memory name should be unique.\n" str.cat
    "MEMORY_REDEFINITION" CompilerError
  endif
  token_value
end

// Parse Functions from code
// Params
//    token_matches:  List[Match]
//    constants:      List[Constant]
//    memories:       List[Memory]
// Return
//    functions: List[Function]
function get_functions ptr ptr ptr -> ptr :

  // Initialize variables
  List.init                     // List[Function]
  List.init                     // List[Token]
  0                             // current_part
  get_function_part_delimiters  // FUNCTION_PART_DELIMITERS
  "" str.copy                   // String buffer for function's name
  List.init                     // return_types
  List.init                     // param_types
  List.init                     // param_names
  0                             // index
  take
    index
    param_names
    param_types
    return_types
    function_name
    FUNCTION_PART_DELIMITERS
    current_part
    tokens
    functions
    token_matches
    constants
    memories
  in

  while index token_matches List.len < do
    // Get current word
    index token_matches List.nth ptr.load
    get_token_from_match
    dup Token.get_value
    take token_value token in

    if
      token_value str.copy str.upper
      current_part FUNCTION_PART_DELIMITERS List.nth str.load
      streq
    do
      current_part 1 +
      FUNCTION_PART_DELIMITERS List.len %
      current_part =

      // Append Function and reset variables when function is fully lexed
      if token_value str.copy str.upper "END" streq do

        // Append 0 token as the exit code for MAIN function
        if function_name str.copy str.upper "MAIN" streq do
          token Token.get_location
          TokenType.INT "0" Token.init
          tokens List.append
        endif

        // Do not allow empty functions
        if tokens List.len 0 == do
          "Function '"
          function_name                                 str.cat
          "' does not have any tokens\n"                str.cat
          "Creation of empty functions is not allowed"  str.cat
          "EMPTY_FUNCTION" CompilerError
        endif

        // Function(function_name, signature, tokens)
        tokens
        return_types param_types param_names Signature.init
        function_name Function.init

        // Append the current function to functions list
        functions List.append

        // Reset variables
        "" str.copy function_name =
        List.init   param_names   =
        List.init   param_types   =
        List.init   return_types  =
        List.init   tokens        =
        index 1 + index =
        continue
      endif

    elif current_part 1 == do
      token_value function_name =
      current_part 1 + current_part =
      // TODO: Check for function redefinition
      // TODO: Check for Constants

    elif current_part 2 == do
      // Enable defining functions that do not return anything without the -> token
      // FUNCTION <name> <param_types> : <function_body> END
      if token_value ":" streq do
        current_part 2 + current_part =
        index 1 + index =
        token_value str.delete
        continue
      endif

      // Split token by colon to name and TokenType
      // Example => parameter_name:int
      ":" token_value str.find
      take param_colon in

      // Only allow named parameters
      if param_colon -1 == do
        "Invalid function parameter '" token_value                                    str.cat
        "'\nFunction parameter must have a colon delimiting its name and TokenType\n" str.cat
        "Example => parameter_name:int"                                                 str.cat
        "SYNTAX_ERROR" CompilerError
      endif

      // Parse parameter's name
      token_value str.copy
      dup param_colon str+ str.empty
      ptr param_names List.append

      // Parse parameter's TokenType
      token_value param_colon 1 + str+
      Signature.type_map ptr
      param_types List.append

    elif current_part 3 == do
      // Append word to return_types if valid TokenType
      token_value Signature.type_map ptr
      return_types List.append
    elif current_part 4 == do
      // Use Constant's value if a Constant exists named as the Token's value
      if constants token_value str.copy constant_exists do

        // Find the constant from list of Constants
        0 take constant_index in
        while constant_index constants List.len < do
          constant_index constants List.nth ptr.load
          take constant in

          if constant Constant.get_name token_value streq do
            constant Constant.get_value str.copy
            token Token.set_value
            TokenType.INT token Token.set_type
            break
          endif
          constant_index 1 + constant_index =
        done
      endif

      // Append the current Token to the list of tokens
      token tokens List.append

    // Not parsing a function
    else
      token Token.delete
      token_value str.delete
    endif
    index 1 + index =
  done

  // Parse the variables for each function
  memories functions parse_variables_for_functions
  functions
end

// Parse Variables for Functions
// Params: List[Function], List[Memory]
function parse_variables_for_functions ptr ptr :
  dup List.len
  0
  take
    index
    len
    functions
    memories
  in

  // Parse Variables for each Function
  while index len < do
    index functions List.nth ptr.load
    memories swap parse_function_variables
    index 1 + index = // index++
  done
end

// Parse Variables in Function
// Params: Function, List[Memory]
// Return: None
function parse_function_variables ptr ptr :
  dup   Function.get_tokens
  over  Function.get_variables
  False
  "PEEK"
  0
  take
    index
    bind_variant
    parsing_variables
    variables
    tokens
    func
    memories
  in

  while index tokens List.len < do
    index tokens List.nth ptr.load
    dup Token.get_value
    take
      token_value
      token
    in

    // Variable parsing block is started with PEEK or TAKE keyword
    if
      token_value str.copy str.upper
      dup  "PEEK" streq
      swap "TAKE" streq
      ||
    do
      True parsing_variables =
      token_value str.copy str.upper bind_variant =

    // IN keyword closes the Variable parsing block
    elif token_value str.copy str.upper "IN" streq do
      False parsing_variables =

    // Every word is considered as a Variable in the block between PEEK/TAKE and IN
    elif parsing_variables do
      if bind_variant "PEEK" streq do
        VarType.PEEK token Token.set_vartype
      elif bind_variant "TAKE" streq do
        VarType.TAKE token Token.set_vartype
      else
        "Unknown VarType '" bind_variant str.cat "'\n" str.cat
        "VALUE_ERROR" CompilerError
      endif

      // Append the variable to the list of function's variables
      token Token.get_type token_value Variable.init
      variables List.append

      // Add the Variable to the list of memories
      func Function.get_name "_"  str.cat
      token_value                 str.cat
      take memory_name in
      if memories memory_name memory_exists not do
        token Token.get_location
        ptr.size // Memory.size
        memory_name
        Memory.init
        memories List.append
      endif

    // If Token is already a known Variable just update its boolean value
    elif token_value variables List.contains_variable do
      VarType.PUSH token Token.set_vartype
    endif
    index 1 + index =
  done
end
