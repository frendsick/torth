// lex.torth - Lexing functions that parses Tokens from code files
include "std"
include "typing"
include "compiler/defs"
include "compiler/utils"
include "compiler/class/Constant"
include "compiler/class/Function"
include "compiler/class/Match"
include "compiler/class/Memory"
include "compiler/class/Op"
include "compiler/class/Token"
include "compiler/class/Signature"

// Parse included files from a code string. Return the list of files.
// Params: code_file (STR), compiler_directory (STR)
// Return: List[str]
memory already_included_files ptr.size end
function get_included_files str str -> ptr :
  ptr list.init list.append
  300
  0
  0   // index
  take
    index
    recursive_include_count
    RECURSIVE_INCLUDE_LIMIT
    included_files
    compiler_directory
  in

  // Iterate over all of the new included files for this loop iteration
  while index included_files list.len < do

    // Save the current file name
    index included_files list.nth str.load
    take file_name in

    // Get includes from the current file
    included_files file_name get_included_files_from_file

    // Save the new included files from this iteration
    included_files =
    index 1 + index = // index++
  done
  included_files
end

// Parse included files from a single file. Return the list of files.
// Params: file_name (STR), included_files (List[str])
// Return: included_files (List[str])
function get_included_files_from_file str ptr -> ptr :
  swap list.copy

  // Add compiler's directory and the 'lib' directory to INCLUDE_PATHS
  list.init
  ""      ptr swap list.append
  "lib/"  ptr swap list.append
  take
    INCLUDE_PATHS
    included_files
    code_file
  in

  // Get the file contents without comments
  INCLUDE_PATHS code_file get_file_name_from_path read_file
  code_file over remove_comments_from_code
  take code_without_comments code in

  // Get the included files from the code to included_files list
  list.init code_file code_without_comments get_token_matches_from_code
  False
  0
  take
    index
    parsing_include
    token_matches
  in

  while index token_matches list.len < do
    // Get Nth word from words list
    index token_matches list.nth ptr.load
    Match.get_value
    take token_value in

    // Parse the file name for the include
    if parsing_include do
      token_value take file_name in

      if file_name int NULL == do
        "INCLUDE keyword used without given file name\n"
        "INCLUDE_ERROR" CompilerError
      endif

      // Get the file name that exists using directories in PATH
      INCLUDE_PATHS file_name get_file_name_from_path
      file_name =

      // Do not save file name if it already is in the parsing_include list
      if file_name included_files list.contains_str do
        False parsing_include =
        index 1 + index =
        continue
      endif

      // Save the file name to included_files
      file_name ptr included_files list.append
      included_files =
      False parsing_include =
    endif

    if token_value str.copy str.upper "INCLUDE" streq do
      True parsing_include =
    endif
    index 1 + index = // index++
  done

  // Clean memory that is not needed any more
  code str.delete
  code_without_comments str.delete

  included_files
end

// Check every directory in PATH and add .torth extension if needed
// Params: file_name: str, INCLUDE_PATHS: List[str]
// Return: file_with_path: str
function get_file_name_from_path str ptr -> str :
  take file_name INCLUDE_PATHS in

  // Get the file name without quotes
  if file_name ptr char.load '"' == do
    file_name get_string_inside_quotes
    file_name =
  endif

  if file_name int NULL == do
    "Cannot include file "
    file_name str.cat
    "\n"      str.cat
    "INCLUDE_ERROR" CompilerError
  endif

  if file_name file_exists do
    file_name return
  endif

  // Iterate over INCLUDE PATHS
  // and check if file_name or file_with_extension exists
  0 take include_index in
  while include_index INCLUDE_PATHS list.len < do
    include_index INCLUDE_PATHS list.nth str.load
    take path_dir in

    // Append file_name to path_dir and check if it exists
    path_dir file_name str.cat
    take file_with_path in
    if file_with_path file_exists do
      file_with_path return
    else
      // Check if the file with .torth extension exists
      file_with_path ".torth" str.cat
      file_with_path =
      if file_with_path file_exists do
        file_with_path return
      endif
    endif
    include_index 1 + include_index =
  done

  "File " file_name       str.cat
  " cannot be imported\n" str.cat
  "INCLUDE_ERROR" CompilerError ""
end

function get_token_matches_from_files ptr -> ptr :
  list.init
  0
  take
    index
    token_matches
    included_files
  in

  // Merge all included files to one code string
  while index included_files list.len < do

    // Read the current file
    index included_files list.nth str.load
    dup read_file
    take code file_name in

    file_name code remove_comments_from_code
    take code_without_comments in

    // Parse Token Matches from the file
    token_matches file_name code_without_comments get_token_matches_from_code
    token_matches =
    index 1 + index = // index++
  done
  token_matches
end

// Get multiline string inside double quotes ""
// Return NULL if there is no string inside quotes
// Params: String which begins with a double quote
// Return: The multiline string between double quotes
function get_string_inside_quotes str -> str :
  str.copy
  take original in

  // Return NULL if the first character is not a double quote
  if original ptr char.load '"' != do
    NULLPTR str return
  endif

  1 while dup original str.len < do
    1 + // index++

    // Return when the other quote is found
    if original over str.char_at '"' == do
      original ptr over ptr+ NULL char swap char.store
      drop
      original 1 str+ return
    endif
  done drop
  NULLPTR str
end

// Get the indexes of the newlines in a string
// Params: string
// Return: List[int]
function get_newline_indexes str -> ptr :
  dup str.len
  list.init
  0
  take
    index
    newline_indexes
    len
    string
  in

  // Iterate over all characters of the string
  while index len < do

    // Save index if the current character is a newline
    if string index str.char_at 10 == do
      index ptr newline_indexes list.append
      newline_indexes =
    endif
    index 1 + index =
  done
  newline_indexes
end

// Remove comments from code
// Params: code (STR), file_name (STR)
// Return: code (STR)
function remove_comments_from_code str str -> str :
  list.init rot rot get_token_matches_from_code
  "" str.copy
  NULL
  0
  take
    index
    comment_line // Removing comment if this is not NULL
    code_buffer
    token_matches
  in

  // Iterate over every word of the code
  while index token_matches list.len < do

    // Load necessary information about the current Token Match
    index token_matches list.nth ptr.load Match.get_value
    index token_matches list.nth ptr.load Match.get_location Location.get_row
    take row token_value in

    // Start removing comments if the current token_value startswith //
    if token_value "//" str.startswith do
      row comment_line =
    endif

    // Stop removing comment if the current token_value is in a new line
    if row comment_line != do
      NULL comment_line =
    endif

    // Continue to next character if removing comments from the line
    if comment_line NULL > do
      index 1 + index =
      continue
    endif

    // Append the current token_value to code_buffer
    code_buffer " " str.cat
    token_value     str.cat
    code_buffer =
    index 1 + index = // index++
  done
  code_buffer
end

// Transfer comparison and calculations related symbols to their text counterparts
// Params: Token match from code (STR)
// Return: Value for Token (STR)
function get_token_value str -> str :
  str.copy
  take token_match in
  if   token_match "="  streq do
    "ASSIGN" return
  elif token_match "==" streq do
    "EQ" return
  elif token_match ">=" streq do
    "GE" return
  elif token_match ">"  streq do
    "GT" return
  elif token_match "<=" streq do
    "LE" return
  elif token_match "<"  streq do
    "LT" return
  elif token_match "-"  streq do
    "MINUS" return
  elif token_match "*"  streq do
    "MUL" return
  elif token_match "!=" streq do
    "NE" return
  elif token_match "+"  streq do
    "PLUS" return
  endif
  token_match str.escape
end

const single_quote 39 end
function get_token_type str -> int :
  take token_value in
  // TODO: TokenType.KEYWORD
  // Boolean => True
  if
    token_value str.copy str.upper
    dup   "TRUE"  streq
    swap  "FALSE" streq
    ||
  do
    TokenType.BOOL return
  // Character => 'a'
  elif
    token_value str.len 3 ==
    token_value ptr char.load single_quote ==
    token_value 2 str.char_at single_quote ==
    && &&
  do
    TokenType.CHAR return
  // Integer => 1337
  elif
    token_value str.is_numeric
  do
    TokenType.INT return
  // Hexadecimal => 0x1337
  elif
    token_value ptr char.load '0' ==
    token_value 1 str.char_at 'x' ==
    &&
  do
    TokenType.INT return
  // String => "This is string\n"
  elif
    token_value ptr char.load                '"' ==
    token_value dup str.len 1 - str.char_at  '"' ==
    &&
  do
  TokenType.STR return
  // uint8 => u69
  elif
    token_value ptr char.load 'u' ==
    token_value 1 str.char_at char.is_numeric
    &&
  do
    TokenType.UINT8 return
  endif
  TokenType.WORD
end

// Generate Token from Match object
// Params: Match
// Return: Token
function get_token_from_match ptr -> ptr :
  dup   Match.get_value
  swap  Match.get_location
  take location match_value in

  // Create Token object
  location
  match_value get_token_value
  dup get_token_type
  swap Token.init
end

function get_op_type ptr -> int :
  Token.get_type
  take token_type in
  if token_type TokenType.BOOL == do
    OpType.PUSH_BOOL return
  elif token_type TokenType.CHAR == do
    OpType.PUSH_CHAR return
  elif token_type TokenType.INT == do
    OpType.PUSH_INT return
  elif token_type TokenType.STR == do
    OpType.PUSH_STR return
  elif token_type TokenType.UINT8 == do
    OpType.PUSH_UINT8 return
  endif
  OpType.INTRINSIC
end

// Functions are made of four parts:
//   1 : name
//   2 : param types
//   3 : return types
//   4 : location
//  (0 : Not lexing a function)
//  FUNCTION_PART_DELIMITERS: List[str] = ["FUNCTION", "->", ":", "END"]
function get_function_part_delimiters -> ptr :
  list.init
  "FUNCTION"  ptr swap list.append
  ""          ptr swap list.append
  "->"        ptr swap list.append
  ":"         ptr swap list.append
  "END"       ptr swap list.append
end

// Get all words from code to a list
// Params
//    code: str
// Return
//    List[str]
function get_words_from_code str -> ptr :
  list.init
  "" str.copy // String buffer for word
  take
    word
    word_list
    code
  in

  0 while
    code over str.char_at
    peek current_char in
    NULL !=
  do
    if
      current_char char.is_whitespace
    do
      // If the word is empty there has been multiple whitespaces in a row
      if word str.len 0 == do
        1 + // index++
        continue
      endif

      // Append current word to word_list
      word str.copy ptr word_list list.append
      word_list =
      word str.empty
    else
      // Append current character to word
      current_char word str.append
    endif
    1 + // index++
  done drop

  // Append the last word if the file ended with a word
  if word str.len 0 > do
    word ptr word_list list.append
    word_list =
  endif
  word_list
end

// Get all token matches from code to a list
// Params
//    code: str
//    file_name: str
// Return
//    List[Match]
function get_token_matches str str -> ptr :
  take code file_name in

  // Get Matches from the whole code
  file_name code get_token_matches_from_code
  take matches in

  // Get Matches from the code without comments
  file_name code remove_comments_from_code
  file_name swap get_token_matches_from_code
  0
  take index matches_without_comments in

  // Return only matches which exist in matches_without_comments
  while index matches list.len < do
    index matches list.nth ptr.load
    Match.get_value
    take match_value in

    // If i >= size of matches_without_comments list then the rest is comments
    if index matches_without_comments list.len >= do
      index matches list.pop
      index 1 + index =
      continue
    endif

    // Compare match_value with corresponding Match.value from matches_without_comments
    index matches_without_comments list.nth ptr.load
    Match.get_value

    if match_value streq not do
      index matches list.pop
      index 1 - index = // index--
    endif
    index 1 + index = // index++
  done
  matches
end

// Get all word matches from code to a list
// Params
//    code: str
//    file_name: str
//    token_matches: str
// Return
//    List[str]
function get_token_matches_from_code str str ptr -> ptr :
  dup get_newline_indexes
  "" str.copy // String buffer for word
  False
  0
  take
    index
    parsing_string
    word
    newline_indexes
    code
    file_name
    token_matches
  in

  while
    code index str.char_at
    peek current_char in
    NULL !=
  do
    // If the word only has one single quote the word is a character definition
    if word "'" streq do
      current_char word str.append
      index 1 + index =
      continue
    endif

    if
      current_char char.is_whitespace
      parsing_string not
      &&
    do
      // If the word is empty there has been multiple whitespaces in a row
      if word str.len 0 == do
        index 1 + index = // index++
        continue
      endif

      // Match(file_name, location)
      index word str.len -
      take word_index in

      newline_indexes word_index file_name get_token_location
      word str.copy Match.init

      // Append current Token Match to token_matches
      token_matches list.append
      token_matches =

      // Reset word
      word str.delete
      "" str.copy word =
    elif current_char '"' == do
      // Toggle parsing_string
      parsing_string not
      parsing_string =
      current_char word str.append
    else
      // Append current character to word
      current_char word str.append
    endif
    index 1 + index = // index++
  done

  // Append the last word if the file ended with a word
  if word str.len 0 > do
    index word str.len -
    newline_indexes swap file_name get_token_location
    word Match.init

    token_matches list.append
    token_matches =
  endif
  token_matches
end

// Calculate position for Token based on it's position from the start of the file
// and the indexes of the newline characters found from the file.
// Params: file_name (STR), position (INT), newline_indexes (List[Int])
// Return: Location
function get_token_location str int ptr -> ptr :
  over
  1
  0
  take
    index
    row
    col
    file_name
    position
    newline_indexes
  in

  // Get Location for found Token Match
  while index newline_indexes list.len < do

    if index 0 > do
      index 1 - newline_indexes list.nth int.load
      take nl_index in

      row 1 + row = // row++
      position nl_index - 1 - col = // col = position - nl_index - 1
    endif

    // Return when the current Token Location is found
    if index newline_indexes list.nth int.load position > do
      col 1 + row file_name str.copy Location.init
      return
    endif
    index 1 + index =
  done

  col 1 + row file_name str.copy Location.init
end

// Parse Constants from code
// Params
//    token_matches: List[Match], constants: List[Constant]
// Return
//    constants: List[Constant]
function get_constants ptr -> ptr :
  list.init
  ""  str.copy        // constant_name
  dup str.copy        // constant_value
  False               // defining_constant
  0                   // index
  take
    index
    defining_constant
    constant_value
    constant_name
    constants
    token_matches
  in

  while index token_matches list.len < do
    index token_matches list.nth ptr.load
    dup   Match.get_value
    swap  Match.get_location
    take location token_value in

    // Parse Constant only when inside Constant block
    if defining_constant do

      // Parse Constant name
      if constant_name str.len 0 == do
        constants token_value get_constants_name
        constant_name =
        index 1 + index = // index++
        continue
      endif

      // Append the current Constant to constants
      if constant_value str.len 0 == do

        // Parse Constant value
        token_value constant_value =

        // Generate Constant object
        location constant_value constant_name Constant.init
        constants list.append constants =

        // Reset variables
        ""    constant_name   =
        ""    constant_value  =
        False defining_constant =
        index 1 + index = // index++
        continue
      endif
    endif

    if token_value str.copy str.upper "CONST" streq do
      True defining_constant =
    endif
    index 1 + index = // index++
  done
  constants token_matches add_enums_to_constants
end

// Parse and add ENUM block contents to a list of Constants.
// Items inside ENUM blocks are interpreted as running integers starting from 0.
// Params
//    token_matches: List[Match]
//    constants: List[Constant]
// Return
//    constants: List[Constant]
function add_enums_to_constants ptr ptr -> ptr :
  get_enum_part_delimiters
  "" str.copy
  NULLPTR
  0
  0
  0
  0
  take
    index
    current_part
    enum_size
    enum_offset
    enum_location
    enum_name
    ENUM_PART_DELIMITERS
    token_matches
    constants
  in

  while index token_matches list.len < do

    // Get the current Token Matches value
    index token_matches list.nth ptr.load
    dup  Match.get_value
    swap Match.get_location

    take location token_value in

    if
      token_value str.copy str.upper
      current_part ENUM_PART_DELIMITERS list.nth str.load
      streq
    do
      current_part 1 +
      ENUM_PART_DELIMITERS list.len %
      current_part =

      if token_value str.copy str.upper "END" streq do

        // Constant(enum_name, enum_size*enum_offset, enum_location)
        enum_location
        enum_size enum_offset * itoa
        enum_name
        Constant.init

        // Append the current Enum to the list of Constants
        constants list.append
        constants =
        index 1 + index =

        // Reset variables
        0 enum_size =
        continue
      endif

    elif current_part 1 == do
      token_value enum_name =
      location enum_location =
      current_part 1 + current_part =

    elif current_part 2 == do
      if token_value str.is_numeric not do
        "'" token_value str.cat
        "' is not a valid offset parameter for ENUM block\n" str.cat
        "VALUE_ERROR" CompilerError
      endif
      token_value atoi enum_offset =
      current_part 1 + current_part =

    elif current_part 3 == do
      "Token '" token_value str.cat
      "' is used in the wrong context when defining '" str.cat
      enum_name str.cat
      "' Enum.\nCheck the syntax of the Enum definition.\n" str.cat
      "VALUE_ERROR" CompilerError

    elif current_part 4 == do
      if constants token_value constant_exists do
        "Constant '" token_value str.cat
        "' is defined multiple times.\n" str.cat
        "Constant names should be unique.\n" str.cat
        "CONST_REDEFINITION" CompilerError
      endif

      // Constant(token_value, enum_size*enum_offset, location)
      location
      enum_size enum_offset * itoa
      token_value
      Constant.init

      // Append the current Enum item to constants
      constants list.append
      constants =
      enum_size 1 + enum_size =
    endif
    index 1 + index = // index++
  done
  constants
end

// Enums are made of four parts:
//   1 : name
//   2 : size
//   3 : offset
//   4 : items
//  (0 : Not lexing Enum)
//  ENUM_PART_DELIMITERS: List[str] = ["ENUM", "", "", ":", "END"]
function get_enum_part_delimiters -> ptr :
  list.init
  "ENUM"  ptr swap list.append
  ""      ptr swap list.append
  ""      ptr swap list.append
  ":"     ptr swap list.append
  "END"   ptr swap list.append
end

// Parse Memories from code
// Params
//    token_matches: List[Match]
//    constants: List[Constant]
// Return
//    memories: List[Memory]
function get_memories ptr ptr -> ptr :
  list.init           // memories
  "" str.copy         // memory_name
  NULL                // memory_size
  False               // defining_memory
  0                   // index
  take
    index
    defining_memory
    memory_size
    memory_name
    memories
    token_matches
    constants
  in

  while index token_matches list.len < do
    index token_matches list.nth ptr.load
    dup   Match.get_value
    swap  Match.get_location
    take location token_value in

    // Parse Memory only when inside Memory block
    if defining_memory do

      // Parse Memory name
      if memory_name str.len 0 == do
        memories token_value get_memory_name
        memory_name =

        index 1 + index = // index++
        continue
      endif

      // Append the current Memory to memories
      if memory_size 0 == do

        // Use the token_value as memory_size if it is numeric
        if token_value str.is_numeric do
          token_value atoi memory_size =
        else
          // Test if Constant exists
          if constants token_value constant_exists do
            constants token_value get_constant
            Constant.get_value atoi
            memory_size =
          else
            "The memory size should be an integer. Got: "
            token_value str.cat
            "VALUE_ERROR" CompilerError
          endif
        endif

        // Generate Memory object
        location memory_size memory_name str.copy Memory.init
        memories list.append memories =

        // Reset variables
        ""    memory_name =
        NULL  memory_size =
        False defining_memory =
        index 1 + index = // index++
        continue
      endif
    endif

    if token_value str.copy str.upper "MEMORY" streq do
      True defining_memory =
    endif
    index 1 + index = // index++
  done
  memories
end

// Check for redefinition of a Constant object
// Params
//    token_value: str
//    memories: List[Constant]
// Return
//    memory_name: str
function get_constants_name str ptr -> str :
  take token_value constants in

  // Overwriting another Memory is not allowed
  if token_value constants list.contains_str do
    "Constant '" token_value str.cat
    "' already exists. Memory name should be unique.\n" str.cat
    "MEMORY_REDEFINITION" CompilerError
  endif
  token_value
end

// Check for redefinition of a Memory object
// Params
//    token_value: str
//    memories: List[Memory]
// Return
//    memory_name: str
function get_memory_name str ptr -> str :
  take token_value memories in

  // Overwriting another Memory is not allowed
  if token_value memories list.contains_str do
    "Memory '" token_value str.cat
    "' already exists. Memory name should be unique.\n" str.cat
    "MEMORY_REDEFINITION" CompilerError
  endif
  token_value
end

// Parse Functions from code
// Params
//    token_matches:  List[Match]
//    constants:      List[Constant]
//    memories:       List[Memory]
// Return
//    functions: List[Function]
function get_functions ptr ptr ptr -> ptr :

  // Initialize variables
  list.init                     // List[Function]
  list.init                     // List[Token]
  0                             // current_part
  get_function_part_delimiters  // FUNCTION_PART_DELIMITERS
  "" str.copy                   // String buffer for function's name
  list.init                     // return_types
  list.init                     // param_types
  0                             // index
  take
    index
    param_types
    return_types
    function_name
    FUNCTION_PART_DELIMITERS
    current_part
    tokens
    functions
    token_matches
    constants
    memories
  in

  while index token_matches list.len < do
    // Get current word
    index token_matches list.nth ptr.load
    get_token_from_match
    dup Token.get_value
    take token_value token in

    if
      token_value str.copy str.upper
      current_part FUNCTION_PART_DELIMITERS list.nth str.load
      streq
    do
      current_part 1 +
      FUNCTION_PART_DELIMITERS list.len %
      current_part =

      // Append Function and reset variables when function is fully lexed
      if token_value str.copy str.upper "END" streq do

        // Append 0 token as the exit code for MAIN function
        if function_name str.copy str.upper "MAIN" streq do
          token Token.get_location
          TokenType.INT "0" Token.init
          tokens list.append
          tokens =
        endif

        // Function(function_name, signature, tokens)
        tokens
        return_types param_types Signature.init
        function_name Function.init

        // Append the current function to functions list
        functions list.append
        functions =

        // Reset variables
        "" str.copy function_name =
        list.init   param_types   =
        list.init   return_types  =
        list.init   tokens        =
        index 1 + index =
        continue
      endif

    elif current_part 1 == do
      token_value function_name =
      current_part 1 + current_part =
      // TODO: Check for function redefinition
      // TODO: Check for Constants

    elif current_part 2 == do
      // Enable defining functions that do not return anything without the -> token
      // FUNCTION <name> <param_types> : <function_body> END
      if token_value ":" streq do
        current_part 2 + current_part =
        index 1 + index =
        token_value str.delete
        continue
      endif

      // Append word to param_types if valid TokenType
      token_value Signature.type_map ptr
      param_types list.append
      param_types =

    elif current_part 3 == do

      // Append word to return_types if valid TokenType
      token_value Signature.type_map ptr
      return_types list.append
      return_types =
    elif current_part 4 == do

      // Use Constant's value if a Constant exists named as the Token's value
      if constants token_value str.copy constant_exists do

        // Find the constant from list of Constants
        0 take constant_index in
        while constant_index constants list.len < do
          constant_index constants list.nth ptr.load
          take constant in

          if constant Constant.get_name token_value streq do
            constant Constant.get_value str.copy
            token Token.set_value
            TokenType.INT token Token.set_type
            break
          endif
          constant_index 1 + constant_index =
        done
      endif

      // Append the current Token to the list of tokens
      token tokens list.append
      tokens =

    // Not parsing a function
    else
      token Token.delete
      token_value str.delete
    endif
    index 1 + index =
  done

  // Parse the variables for each function
  memories functions parse_variables_for_functions
  functions
end

// Parse Variables for Functions
// Params: List[Function], List[Memory]
function parse_variables_for_functions ptr ptr :
  dup list.len
  0
  take
    index
    len
    functions
    memories
  in

  // Parse Variables for each Function
  while index len < do
    index functions list.nth ptr.load
    memories swap parse_function_variables
    index 1 + index = // index++
  done
end

// Parse Variables in Function
// Params: Function, List[Memory]
// Return: None
function parse_function_variables ptr ptr :
  dup Function.get_tokens
  list.init
  False
  "PEEK"
  0
  take
    index
    bind_variant
    parsing_variables
    variables
    tokens
    func
    memories
  in

  while index tokens list.len < do
    index tokens list.nth ptr.load
    dup Token.get_value
    take
      token_value
      token
    in

    // Variable parsing block is started with PEEK or TAKE keyword
    if
      token_value str.copy str.upper
      dup  "PEEK" streq
      swap "TAKE" streq
      ||
    do
      True parsing_variables =
      token_value str.copy str.upper bind_variant =

    // IN keyword closes the Variable parsing block
    elif token_value str.copy str.upper "IN" streq do
      False parsing_variables =

    // Every word is considered as a Variable in the block between PEEK/TAKE and IN
    elif parsing_variables do
      if bind_variant "PEEK" streq do
        VarType.PEEK token Token.set_vartype
      elif bind_variant "TAKE" streq do
        VarType.TAKE token Token.set_vartype
      else
        "Unknown VarType '" bind_variant str.cat "'\n" str.cat
        "VALUE_ERROR" CompilerError
      endif
      token_value ptr variables list.append
      variables =

      // Add the Variable to the list of memories
      func Function.get_name "_"  str.cat
      token_value                 str.cat
      take memory_name in
      if memories memory_name memory_exists not do
        token Token.get_location
        ptr.size // Memory.size
        memory_name
        Memory.init
        memories list.append
        memories =
      endif

    // If Token is already a known Variable just update its boolean value
    elif token_value variables list.contains_str do
      VarType.PUSH token Token.set_vartype
    endif
    index 1 + index =
  done
end
